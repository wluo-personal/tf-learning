{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1655d2b7",
   "metadata": {},
   "source": [
    "This notebook focus on Actor-Critc and A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cf64ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.613170Z",
     "start_time": "2023-03-17T18:11:29.591141Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# disable tensorflow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tqdm\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293f6b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.618678Z",
     "start_time": "2023-03-17T18:11:30.614728Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "initial_state, _ = env.reset()\n",
    "initial_state_shape = initial_state.shape\n",
    "action_space = env.action_space.n\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "gamma = 1.0\n",
    "lr = 0.01\n",
    "step_length = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a500b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T15:45:52.459819Z",
     "start_time": "2023-03-15T15:45:52.453876Z"
    }
   },
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1719071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.740707Z",
     "start_time": "2023-03-17T18:11:30.619610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model will use basic Actor-Critc (A2C with baseline)\n",
    "\"\"\"\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.layers.Input(shape=initial_state_shape)\n",
    "    hidden = tf.keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    actions = tf.keras.layers.Dense(action_space, activation=tf.keras.activations.softmax)(hidden)\n",
    "    value = tf.keras.layers.Dense(1, activation=None)(hidden)\n",
    "    outs = [actions, value]\n",
    "    return tf.keras.Model(inputs, outs)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045086a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T15:51:42.446877Z",
     "start_time": "2023-03-15T15:51:42.443577Z"
    }
   },
   "source": [
    "# define data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7be422e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.746728Z",
     "start_time": "2023-03-17T18:11:30.742172Z"
    }
   },
   "outputs": [],
   "source": [
    "def _next_step(action):\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, np.float32), np.array(done, np.int32))\n",
    "\n",
    "def tf_next_step(action):\n",
    "    return tf.numpy_function(_next_step, [action], (tf.float32, tf.float32, tf.int32))\n",
    "\n",
    "\n",
    "def run_step(start_state, model, step_length):\n",
    "    rewards = tf.constant(0.0, tf.float32)\n",
    "    values = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    actions = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n",
    "    action_probs = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    state = start_state\n",
    "    for t in tf.range(step_length):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_output, value = model(state)\n",
    "        action = tf.random.categorical(tf.math.log(action_output), 1, dtype=tf.int32) \n",
    "        action = tf.squeeze(action)\n",
    "        ## option to try math\n",
    "#         action = tf.math.argmax(tf.squeeze(action_output), output_type=tf.int32)\n",
    "        action_prob = action_output[0, action]\n",
    "    \n",
    "        \n",
    "        state, reward, done = tf_next_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        rewards += reward     \n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "        actions = actions.write(t, action)\n",
    "        action_probs = action_probs.write(t, tf.squeeze(action_prob))\n",
    "        done = tf.cast(done, tf.bool)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    _, next_value = model(tf.expand_dims(state, 0))\n",
    "    next_value = tf.squeeze(next_value)\n",
    "    values = values.stack()\n",
    "    actions = actions.stack()\n",
    "    action_probs = action_probs.stack()\n",
    "    return values, actions, action_probs, rewards, next_value, state, done\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41de31ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.806385Z",
     "start_time": "2023-03-17T18:11:30.748386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/envs/tf_211/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(26,), dtype=float32, numpy=\n",
       " array([-0.00321881, -0.0064002 , -0.00191115, -0.00574407, -0.01122334,\n",
       "        -0.01608   , -0.00854622, -0.00299684,  0.00040344, -0.05456816,\n",
       "         0.00120498, -0.04965396, -0.11863778, -0.18882552, -0.2588526 ,\n",
       "        -0.329962  , -0.2636674 , -0.19884075, -0.13876355, -0.21346955,\n",
       "        -0.1568311 , -0.23310043, -0.31304407, -0.25908795, -0.20702438,\n",
       "        -0.15216939], dtype=float32)>,\n",
       " <tf.Tensor: shape=(26,), dtype=int32, numpy=\n",
       " array([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1], dtype=int32)>,\n",
       " <tf.Tensor: shape=(26,), dtype=float32, numpy=\n",
       " array([0.50302094, 0.48568046, 0.50213146, 0.51379186, 0.5261309 ,\n",
       "        0.4612352 , 0.47313854, 0.48483455, 0.49768218, 0.48926473,\n",
       "        0.49598694, 0.5098852 , 0.51921064, 0.5274372 , 0.53492934,\n",
       "        0.45787916, 0.46908158, 0.47969836, 0.5104296 , 0.4811098 ,\n",
       "        0.5090946 , 0.5174549 , 0.47426614, 0.4850101 , 0.49475515,\n",
       "        0.49573106], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=26.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-0.2356856>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.13642518,  0.4086448 , -0.21144535, -0.88077337], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=bool, numpy=True>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state, _ = env.reset()\n",
    "result = run_step(_state, model, 50)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a96751",
   "metadata": {},
   "source": [
    "# define returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afed861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.814585Z",
     "start_time": "2023-03-17T18:11:30.807661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(26,), dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(26,), dtype=float32, numpy=\n",
       " array([26., 25., 24., 23., 22., 21., 20., 19., 18., 17., 16., 15., 14.,\n",
       "        13., 12., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_returns(total_rewards):\n",
    "    \"\"\"\n",
    "    Input: total_rewards is a value\n",
    "    Output: \n",
    "        discount_array: array of discount rate.\n",
    "            index i to the (end timestamp + 1) discount rate\n",
    "        returns: array of discounted returns\n",
    "            index i means the returns between index i to the index(end timestamp + 1)\n",
    "    \"\"\"\n",
    "    total_rewards = tf.cast(total_rewards, tf.int32)\n",
    "    rewards_array = tf.ones(shape=(total_rewards,), dtype=tf.float32)\n",
    "    discount_array = rewards_array * gamma\n",
    "    discount_array = tf.math.cumprod(discount_array , reverse=True) \n",
    "    returns_array = tf.math.cumsum(discount_array / gamma, reverse=True)\n",
    "    \n",
    "    return discount_array, returns_array\n",
    "\n",
    "rewards = result[3]\n",
    "get_returns(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a873d05",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361dd554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.819323Z",
     "start_time": "2023-03-17T18:11:30.815605Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "# huber can make sure the actor and critic loss is at the same magnitude\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_discount_rate(values):\n",
    "    \"\"\"\n",
    "    Used for TD learning. As the time step increase the loss will be increased. This method will try to reduce the increasing loss\n",
    "    caused in incrasing time step.\n",
    "    \n",
    "    t1,t2,t3,...tn -> [1/n, 1/n-1, 1/n-2, ... 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    discount = tf.ones_like(values, dtype=tf.float32)\n",
    "    discount = 1.0 / tf.math.cumsum(discount, reverse=True)\n",
    "    discount = tf.math.pow(discount, 1.0)\n",
    "    return discount\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss(returns, values, action_probs, value_next, discount_array):\n",
    "    \"\"\"\n",
    "    Policy part ---------\n",
    "    V(St) = E(Q) = Pi(St, At1; theta) * Q(St, At1) + ...\n",
    "    G denote gradient W.R.T theta\n",
    "    \n",
    "    \n",
    "    G(V(St)) = G[Pi(St, At1; theta) * Q(St, At1) + ...]\n",
    "                approximate= Pi(St, A1)* G(logpi(St, At1; theta) * Q) + ...  # chain rule G(logpi) = 1/pi * G(pi)\n",
    "                = E[ G(logpi * Q) ] # Pi(St, A) is the PDF, so this is the expectation\n",
    "              [1]  approximate= G(logpi * Q)  # monte carlo approximation\n",
    "              [2]  = G(logpi * (Q - baseline))  where baseline can be V. This is the A2C\n",
    "                  Qt can be approximate by Yt\n",
    "              Yt = gamma^T * Q(T) + r + gamma*r + gamma^2*r + ...\n",
    "              \n",
    "    Critic Part TD learning -----------\n",
    "    Qt = discounted_ovserved + QT\n",
    "            \n",
    "    \"\"\"\n",
    "    loss_dis_rate = get_loss_discount_rate(values)\n",
    "    \n",
    "    action_probs = tf.squeeze(action_probs)\n",
    "    logpi = tf.math.log( tf.clip_by_value(action_probs, eps, 1.0))\n",
    "    values = tf.squeeze(values)\n",
    "    Yt = returns + discount_array * value_next\n",
    "\n",
    "    \n",
    "    # this is the negative gradient instead of loss\n",
    "    loss_actor = - logpi * (Yt - values)\n",
    "#     loss_actor = - logpi * Yt\n",
    "    loss_actor = tf.reduce_mean(loss_actor)\n",
    "    # critic\n",
    "    loss_critic = loss_func(tf.expand_dims(Yt*loss_dis_rate, 1), tf.expand_dims(values*loss_dis_rate,1))\n",
    "#     loss_critic = loss_func(tf.expand_dims(Yt, 1), tf.expand_dims(values,1))\n",
    "    loss = loss_actor + loss_critic\n",
    "    return loss, loss_actor, loss_critic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b72202c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.829861Z",
     "start_time": "2023-03-17T18:11:30.820481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=10.357225>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.37497>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9822549>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, actions, action_probs, rewards, next_value, state, done = result\n",
    "discount_array, returns_array = get_returns(rewards)\n",
    "calculate_loss(returns_array, values, action_probs, next_value, discount_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a2689",
   "metadata": {},
   "source": [
    "# train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4f1820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.835189Z",
     "start_time": "2023-03-17T18:11:30.830872Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def run_train_step(model, optimizer, start_state, step_length):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        STEP_RES = run_step(start_state, model, step_length)\n",
    "        values, actions, action_probs, rewards, next_value, state, done = STEP_RES\n",
    "        discount_array, returns_array = get_returns(rewards)\n",
    "        loss, loss_actor, loss_critic = calculate_loss(returns_array, values, action_probs, next_value, discount_array)\n",
    "    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "    return STEP_RES, loss, loss_actor, loss_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ecf721e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:11:30.924847Z",
     "start_time": "2023-03-17T18:11:30.836156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(21,), dtype=float32, numpy=\n",
       "  array([ 0.00166062, -0.06392872, -0.00058175,  0.00368372, -0.00185627,\n",
       "         -0.06943968, -0.14092867, -0.21284667, -0.14633742, -0.08269417,\n",
       "         -0.01806045, -0.09361966, -0.16815835, -0.10802558, -0.04622431,\n",
       "         -0.12375565, -0.20051135, -0.27872792, -0.2243101 , -0.30496684,\n",
       "         -0.38762915], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(21,), dtype=int32, numpy=\n",
       "  array([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0],\n",
       "        dtype=int32)>,\n",
       "  <tf.Tensor: shape=(21,), dtype=float32, numpy=\n",
       "  array([0.49703497, 0.48947832, 0.50239354, 0.48430508, 0.49884677,\n",
       "         0.5110986 , 0.5201013 , 0.47168228, 0.4822488 , 0.4925978 ,\n",
       "         0.4968004 , 0.5075302 , 0.48327962, 0.49349865, 0.4953855 ,\n",
       "         0.50627714, 0.51570725, 0.47583163, 0.51381725, 0.5223433 ,\n",
       "         0.46964854], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=21.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=-0.34011278>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.07370707,  0.6212367 , -0.22481671, -1.3316582 ], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(), dtype=bool, numpy=True>),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.527287>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.564222>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.96306455>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state, _ = env.reset()\n",
    "run_train_step(model, optimizer, _state, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4a1c9",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3289955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:13:26.551040Z",
     "start_time": "2023-03-17T18:11:30.926202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 118/10000 [01:55<2:41:22,  1.02it/s, current_reward=1000, loss=236, running_rewards=480]\n"
     ]
    }
   ],
   "source": [
    "min_epoch = 100\n",
    "max_epoch = 10000\n",
    "step_length = 1000\n",
    "thred = 475\n",
    "max_steps_per_epoch = 600\n",
    "running_rewards = collections.deque(maxlen=min_epoch)\n",
    "all_rewards = []\n",
    "all_running_rewards = []\n",
    "t = tqdm.trange(max_epoch)\n",
    "for i in t:\n",
    "    start_state, _ = env.reset()\n",
    "    cur_step = 0\n",
    "    epoch_reward = 0\n",
    "    while cur_step < max_steps_per_epoch:\n",
    "        STEP_RES, loss, loss_actor, loss_critic = run_train_step(model, optimizer, start_state, step_length)\n",
    "        cur_step += step_length\n",
    "        values, actions, action_probs, rewards, next_value, state, done = STEP_RES\n",
    "        epoch_reward += int(tf.reduce_sum(rewards))\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    running_rewards.append(epoch_reward)\n",
    "    avg_reward = statistics.mean(running_rewards)\n",
    "    all_rewards.append(epoch_reward)\n",
    "    all_running_rewards.append(avg_reward)\n",
    "    t.set_postfix(running_rewards=avg_reward, current_reward=epoch_reward, loss=float(loss))\n",
    "    if avg_reward > thred and i > min_epoch:\n",
    "        break\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ba55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf211",
   "language": "python",
   "name": "tf211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
