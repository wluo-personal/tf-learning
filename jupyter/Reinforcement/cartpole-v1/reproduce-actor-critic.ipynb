{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0925552e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.296202Z",
     "start_time": "2023-03-17T18:02:01.286133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 18:02:01.389767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 18:02:01.470855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:01.470872: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-17 18:02:01.848540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:01.848623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:01.848629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tqdm\n",
    "import collections, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae49ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.302807Z",
     "start_time": "2023-03-17T18:02:02.298022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "n_hidden_unit = 128\n",
    "n_action_space = env.action_space.n\n",
    "initial_state, _ = env.reset()\n",
    "initial_state_shape = initial_state.shape\n",
    "gamma = 0.99\n",
    "print(f\"initial state shape: {initial_state_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e059304",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92943b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.437209Z",
     "start_time": "2023-03-17T18:02:02.303856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " common (Dense)                 (None, 128)          640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " output_action_logits (Dense)   (None, 2)            258         ['common[0][0]']                 \n",
      "                                                                                                  \n",
      " output_value (Dense)           (None, 1)            129         ['common[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 18:02:02.389512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-17 18:02:02.389673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389732: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389756: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389803: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-17 18:02:02.389856: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-17 18:02:02.390008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    The input will be the state\n",
    "    The output will be \n",
    "    (1) action logits, shape 2\n",
    "    (2) value, shape 1\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=initial_state_shape)\n",
    "    comm = tf.keras.layers.Dense(n_hidden_unit, activation=\"relu\", name=\"common\")(inputs)\n",
    "    action_logits = tf.keras.layers.Dense(n_action_space, activation=None, name=\"output_action_logits\")(comm)\n",
    "    value = tf.keras.layers.Dense(1, activation=None, name=\"output_value\")(comm)\n",
    "    return tf.keras.Model(inputs, [action_logits, value])\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17c7ec",
   "metadata": {},
   "source": [
    "# Design training data collection -- very important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f75ddf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.443477Z",
     "start_time": "2023-03-17T18:02:02.438599Z"
    }
   },
   "outputs": [],
   "source": [
    "def env_step(action: int):\n",
    "    \"\"\"\n",
    "    env.step cannot be call inside tf graph, so we need to convert it to tf function.\n",
    "    \"\"\"\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    return (\n",
    "        state.astype(np.float32), \n",
    "        np.array(reward,dtype=np.float32),\n",
    "        np.array(done, dtype=np.int32)\n",
    "    )\n",
    "\n",
    "def tf_env_step(action):\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.float32, tf.int32])\n",
    "\n",
    "def run_epoch(initial_state, max_steps, model):\n",
    "    \"\"\"\n",
    "    return action probs, estimated values, actual rewards\n",
    "    \"\"\"\n",
    "    action_probs = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    estimated_values = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    state = initial_state\n",
    "    # tf.range make sure this can be called in a tf graph\n",
    "    for t in tf.range(max_steps):\n",
    "        state = tf.expand_dims(state, 0) # add batch dimension\n",
    "        action_logit, estimated_value = model(state)\n",
    "        # sample the action\n",
    "        action = tf.random.categorical(action_logit, num_samples=1, dtype=None, seed=None, name=None)[0,0]\n",
    "        action_prob = tf.keras.activations.softmax(action_logit) # the first zero is the batch dim\n",
    "        \n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        action_probs = action_probs.write(t, tf.squeeze(action_prob)[action])\n",
    "        estimated_values = estimated_values.write(t, tf.squeeze(estimated_value))\n",
    "        rewards = rewards.write(t, tf.squeeze(reward))\n",
    "        \n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "    action_probs = action_probs.stack()\n",
    "    estimated_values = estimated_values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    return action_probs, estimated_values, rewards\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b925b",
   "metadata": {},
   "source": [
    "## test the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3285b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.489805Z",
     "start_time": "2023-03-17T18:02:02.445494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/envs/tf_211/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(18,), dtype=float32, numpy=\n",
       " array([0.4990992 , 0.50119716, 0.49918973, 0.5014183 , 0.4993501 ,\n",
       "        0.49843   , 0.5020509 , 0.4981027 , 0.49774188, 0.50262576,\n",
       "        0.49747005, 0.5027613 , 0.4974178 , 0.50269586, 0.4978886 ,\n",
       "        0.49761343, 0.49751216, 0.49734417], dtype=float32)>,\n",
       " <tf.Tensor: shape=(18,), dtype=float32, numpy=\n",
       " array([-0.01557202, -0.06963612, -0.01391116, -0.06862514, -0.01309672,\n",
       "        -0.06810703, -0.12750013, -0.06743591, -0.1277196 , -0.18818456,\n",
       "        -0.12875366, -0.19022927, -0.13185446, -0.19431609, -0.13765489,\n",
       "        -0.2005909 , -0.26414856, -0.32797354], dtype=float32)>,\n",
       " <tf.Tensor: shape=(18,), dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state, _ = env.reset()\n",
    "A = run_epoch(initial_state, 500, model)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1686003",
   "metadata": {},
   "source": [
    "# Calculate returns from rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39520c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.494643Z",
     "start_time": "2023-03-17T18:02:02.491089Z"
    }
   },
   "outputs": [],
   "source": [
    "# this will be called inside a tf function\n",
    "def calculate_returns(rewards, normalize=tf.constant(True)):\n",
    "    returns = tf.TensorArray(np.float32, size=0, dynamic_size=True)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    # cannot use rewards.shape since it will cause None issue. The tf.shape will handle this problem\n",
    "    for t in tf.range(tf.shape(rewards)[0]-1, -1, -1):\n",
    "        discounted_sum = discounted_sum * gamma + rewards[t]\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(t, discounted_sum)\n",
    "    returns = returns.stack()\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns) + eps)\n",
    "    return returns\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b39ae",
   "metadata": {},
   "source": [
    "## test the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3b3e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.512077Z",
     "start_time": "2023-03-17T18:02:02.495699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw returns: [ 1.5946827   1.4170172   1.2375572   1.0562843   0.8731804   0.6882269\n",
      "  0.5014053   0.3126966   0.12208184 -0.07045836 -0.26494336 -0.461393\n",
      " -0.65982693 -0.86026525 -1.0627283  -1.2672364  -1.4738101  -1.6824704 ]\n",
      "mean: 2.6490953430879927e-08\n",
      "std: 1.0\n"
     ]
    }
   ],
   "source": [
    "r = calculate_returns(A[-1])\n",
    "print(f\"raw returns: {r}\")\n",
    "print(f\"mean: {tf.math.reduce_mean(r)}\")\n",
    "print(f\"std: {tf.math.reduce_std(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1ec63",
   "metadata": {},
   "source": [
    "# Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b88523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.516354Z",
     "start_time": "2023-03-17T18:02:02.513237Z"
    }
   },
   "outputs": [],
   "source": [
    "# The loss will be called inside tf function\n",
    "huber = tf.losses.Huber(reduction=tf.losses.Reduction.SUM) # the SUM_OVER_BATCH will normalize by batch size\n",
    "def compute_loss(action_probs, estimated_values, actual_values):\n",
    "#     expand dim - this is extremly important. Without below, the loss is wrong and the model will not learn anything\n",
    "    action_probs, estimated_values, actual_values = [\n",
    "        tf.cond(tf.cast(x.shape.ndims==1, tf.bool), \n",
    "                lambda: tf.expand_dims(x,1), \n",
    "                lambda: x\n",
    "               )  for x in (action_probs, estimated_values, actual_values)]\n",
    "    \n",
    "    \n",
    "    advantage = actual_values - estimated_values\n",
    "#     advantage = actual_values\n",
    "    log_action = tf.math.log(tf.clip_by_value(action_probs, eps, 1e10))\n",
    "    loss_actor = - tf.math.reduce_sum(advantage * log_action)\n",
    "    loss_huber = huber(estimated_values, actual_values)\n",
    "    loss = loss_actor + loss_huber\n",
    "    \n",
    "    return loss, loss_actor, loss_huber\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb14728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T15:03:39.911371Z",
     "start_time": "2023-03-08T15:03:39.894632Z"
    }
   },
   "source": [
    "## test the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4444df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.533113Z",
     "start_time": "2023-03-17T18:02:02.517715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.025241>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5992051>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.426036>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = compute_loss(A[0], A[1], calculate_returns(A[-1]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7625068",
   "metadata": {},
   "source": [
    "# Define train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812c7c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:02.538804Z",
     "start_time": "2023-03-17T18:02:02.534333Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def train_step(initial_state, max_steps, model, optimizer):\n",
    "    \"\"\"\n",
    "    1. run epoch to collect data\n",
    "    2. calculate loss\n",
    "    3. update weights\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. run epoch to collect data\n",
    "        action_probs, estimated_values, rewards = run_epoch(initial_state, max_steps, model)\n",
    "        actual_values = calculate_returns(rewards)\n",
    "        \n",
    "        # 2. calculate loss\n",
    "        loss, loss_actor, loss_huber = compute_loss(action_probs, estimated_values, actual_values)\n",
    "        \n",
    "        \n",
    "    # calculate gradient\n",
    "    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "    total_rewards = tf.math.reduce_sum(tf.squeeze(rewards))\n",
    "    A = (loss, loss_actor, loss_huber)\n",
    "    return total_rewards, A\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d22a1",
   "metadata": {},
   "source": [
    "## test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a0b073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:04.110311Z",
     "start_time": "2023-03-17T18:02:02.540121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wei/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=14.0>,\n",
       " (<tf.Tensor: shape=(), dtype=float32, numpy=6.7945375>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2778747>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.5166626>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state, _ = env.reset()\n",
    "train_step(initial_state, 500, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fedf77",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e788eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:32.332465Z",
     "start_time": "2023-03-17T18:02:04.111741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 459/10000 [00:27<09:35, 16.59it/s, avg_rewards=323, current_rewards=222] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m initial_state, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m     10\u001B[0m initial_state \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconstant(initial_state, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m---> 11\u001B[0m epoch_reward, loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m epoch_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(epoch_reward)\n\u001B[1;32m     13\u001B[0m rewards_history\u001B[38;5;241m.\u001B[39mappend(epoch_reward)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[1;32m    133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1741\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1744\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1745\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1747\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m     args,\n\u001B[1;32m   1749\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1750\u001B[0m     executing_eagerly)\n\u001B[1;32m   1751\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    377\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 378\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    387\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    391\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf_211/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "min_epochs_to_run = 100\n",
    "max_epochs_to_run = 10000\n",
    "max_steps_per_epoch = 500\n",
    "reward_thred = 475\n",
    "t = tqdm.trange(max_epochs_to_run)\n",
    "q = collections.deque(maxlen=100)\n",
    "rewards_history = []\n",
    "for i in t:\n",
    "    initial_state, _ = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    epoch_reward, loss = train_step(initial_state, max_steps_per_epoch, model, optimizer)\n",
    "    epoch_reward = int(epoch_reward)\n",
    "    rewards_history.append(epoch_reward)\n",
    "    q.append(epoch_reward)\n",
    "    avg_rewards = statistics.mean(q)\n",
    "    t.set_postfix(current_rewards=epoch_reward, avg_rewards=avg_rewards)\n",
    "    \n",
    "    if avg_rewards > reward_thred and i > min_epochs_to_run:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edec746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T18:02:32.333587Z",
     "start_time": "2023-03-17T18:02:32.333579Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6d200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf211",
   "language": "python",
   "name": "tf211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
