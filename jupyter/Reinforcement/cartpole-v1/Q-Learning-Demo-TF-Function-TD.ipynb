{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1655d2b7",
   "metadata": {},
   "source": [
    "This notebook is for Q learning. The multi-timestep should be taken better care of under this case. Train on batch multiple TD may not be a good idea in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cf64ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.227121Z",
     "start_time": "2023-03-23T16:03:04.190614Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 16:03:04.319007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 16:03:04.400651: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:04.400666: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-23 16:03:04.781331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:04.781370: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:04.781374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# disable tensorflow logging\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tqdm\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec5b7db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.232129Z",
     "start_time": "2023-03-23T16:03:05.228818Z"
    }
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, max_step=1000):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.max_step = max_step\n",
    "        self.cur_step = 1\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "    def reset(self):\n",
    "        self.cur_step = 1\n",
    "        initial_state, info = self.env.reset()\n",
    "        initial_state = self.add_step_into_state(initial_state)\n",
    "        return initial_state, info\n",
    "    \n",
    "    def add_step_into_state(self, state):\n",
    "#         state = np.concatenate([state, np.array([self.cur_step/self.max_step])])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.cur_step += 1\n",
    "        state, reward, done, _, _ = self.env.step(action)\n",
    "        if self.cur_step > self.max_step:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "            self.cur_step = self.max_step + 1\n",
    "        else:\n",
    "            if done:\n",
    "                reward = 1.0\n",
    "        state = self.add_step_into_state(state)\n",
    "        return state, reward, done, _, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293f6b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.279986Z",
     "start_time": "2023-03-23T16:03:05.232990Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = Env()\n",
    "initial_state, _ = env.reset()\n",
    "initial_state_shape = initial_state.shape\n",
    "action_space = env.action_space.n\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "gamma = 0.99\n",
    "lr = 0.0003\n",
    "step_length = 50\n",
    "use_dueling = True \n",
    "\"\"\"Q learning training is much harder than policy gradient\"\"\"\n",
    "\n",
    "if use_dueling:\n",
    "    lr = 0.00001\n",
    "    gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a500b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T15:45:52.459819Z",
     "start_time": "2023-03-15T15:45:52.453876Z"
    }
   },
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1719071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.395185Z",
     "start_time": "2023-03-23T16:03:05.280977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          8320        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            258         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None,)             0           ['dense_2[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.negative (TFOpLambda)  (None,)              0           ['tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " outs (Add)                     (None, 2)            0           ['dense_2[0][0]',                \n",
      "                                                                  'dense_3[0][0]',                \n",
      "                                                                  'tf.math.negative[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,027\n",
      "Trainable params: 9,027\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 16:03:05.337520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-23 16:03:05.337715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337805: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-23 16:03:05.337914: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-23 16:03:05.338169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model will use basic Q-learning\n",
    "\"\"\"\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.layers.Input(shape=initial_state_shape)\n",
    "    hidden = tf.keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    hidden = tf.keras.layers.Dense(128, activation=\"relu\")(hidden)\n",
    "    outs = tf.keras.layers.Dense(action_space, activation=None)(hidden)\n",
    "    return tf.keras.Model(inputs, outs)\n",
    "\n",
    "def get_dueling_model():\n",
    "    \"\"\"\n",
    "    A = Q - S\n",
    "    Q = A + S - mean(A)\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=initial_state_shape)\n",
    "    hidden = tf.keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    hidden = tf.keras.layers.Dense(128, activation=\"relu\")(hidden)\n",
    "    A = tf.keras.layers.Dense(action_space, activation=None)(hidden)\n",
    "    S = tf.keras.layers.Dense(1, activation=None)(hidden)\n",
    "    A_mean = tf.math.reduce_mean(A, axis=1, name=\"mean\")\n",
    "    outs = tf.keras.layers.Add(name=\"outs\")([A, S, -A_mean])\n",
    "    \n",
    "    return tf.keras.Model(inputs, outs)\n",
    "    \n",
    "if use_dueling:\n",
    "    model = get_dueling_model()\n",
    "else:\n",
    "    model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045086a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T15:51:42.446877Z",
     "start_time": "2023-03-15T15:51:42.443577Z"
    }
   },
   "source": [
    "# define data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d600bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.401623Z",
     "start_time": "2023-03-23T16:03:05.396902Z"
    }
   },
   "outputs": [],
   "source": [
    "def _run_step_numpy(action):\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, dtype=np.float32), np.array(done, dtype=np.int32))\n",
    "\n",
    "def run_step_tf(action):\n",
    "    return tf.numpy_function(_run_step_numpy, [action], (tf.float32, tf.float32, tf.int32))\n",
    "\n",
    "def run_step(start_state, model, step_length):\n",
    "\n",
    "    values = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    actions = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    done = tf.constant(True, dtype=tf.bool)\n",
    "    done_shape = done.shape\n",
    "    \n",
    "    state = start_state\n",
    "    for t in tf.range(step_length):\n",
    "        q_output = model(tf.expand_dims(state, 0))\n",
    "        q_output = tf.squeeze(q_output)\n",
    "        action = tf.math.argmax(q_output, output_type=tf.int32)\n",
    "        \n",
    "        value = q_output[action]\n",
    "        state, reward, done = run_step_tf(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        actions = actions.write(t, action)\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "        rewards = rewards.write(t, reward)\n",
    "        done = tf.cast(done, dtype=tf.bool)\n",
    "        done.set_shape(done_shape)\n",
    "        if done:\n",
    "            break\n",
    "    next_value = model(tf.expand_dims(state, 0))\n",
    "    next_value = tf.squeeze(next_value)\n",
    "    next_value = tf.math.reduce_max(next_value)\n",
    "    \n",
    "     \n",
    "    actions = actions.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    return values, actions, rewards, next_value, state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0765783e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.556244Z",
     "start_time": "2023-03-23T16:03:05.402550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/anaconda3/envs/tf_211/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "state_, _ = env.reset()\n",
    "result = run_step(state_, model, 100)\n",
    "values, actions, rewards, next_value, state, done = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a96751",
   "metadata": {},
   "source": [
    "# define returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9412eb79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.560717Z",
     "start_time": "2023-03-23T16:03:05.557404Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_returns(rewards_array, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Input: total_rewards is a value\n",
    "    Output: \n",
    "        discount_array: array of discount rate.\n",
    "            index i to the (end timestamp + 1) discount rate\n",
    "        returns: array of discounted returns\n",
    "            index i means the returns between index i to the index(end timestamp + 1)\n",
    "    \"\"\"\n",
    "    rewards_array = tf.squeeze(rewards_array)\n",
    "    gamma = tf.constant(gamma, tf.float32)\n",
    "    discounted_return = tf.constant(0.0, tf.float32)\n",
    "    dshape = discounted_return.shape\n",
    "    returns = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    for idx in tf.range(tf.shape(rewards_array)[0] -1, -1, -1):\n",
    "        discounted_return = gamma * discounted_return + rewards_array[idx]\n",
    "        discounted_return.set_shape(dshape)\n",
    "        returns = returns.write(idx, discounted_return)\n",
    "    discount_array = tf.ones_like(rewards_array, dtype=tf.float32) * gamma\n",
    "    discount_array = tf.math.cumprod(discount_array, reverse=True)\n",
    "    returns = returns.stack()\n",
    "    \n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (eps + tf.math.reduce_std(returns))\n",
    "    \n",
    "    return discount_array, returns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232c4638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.631542Z",
     "start_time": "2023-03-23T16:03:05.561659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(48,), dtype=float32, numpy=\n",
       " array([0.61729044, 0.6235257 , 0.6298239 , 0.63618577, 0.64261186,\n",
       "        0.64910287, 0.65565944, 0.6622822 , 0.66897196, 0.6757292 ,\n",
       "        0.6825548 , 0.68944925, 0.6964134 , 0.7034479 , 0.7105534 ,\n",
       "        0.7177307 , 0.7249805 , 0.7323035 , 0.7397005 , 0.74717224,\n",
       "        0.75471944, 0.7623429 , 0.7700433 , 0.77782154, 0.7856783 ,\n",
       "        0.79361445, 0.80163074, 0.809728  , 0.8179071 , 0.8261688 ,\n",
       "        0.8345139 , 0.8429433 , 0.8514579 , 0.8600585 , 0.8687459 ,\n",
       "        0.8775211 , 0.88638496, 0.89533836, 0.90438217, 0.91351736,\n",
       "        0.9227448 , 0.9320654 , 0.9414802 , 0.95099014, 0.9605961 ,\n",
       "        0.97029907, 0.98010004, 0.99      ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(48,), dtype=float32, numpy=\n",
       " array([ 1.5685304 ,  1.5116849 ,  1.4542651 ,  1.3962656 ,  1.3376802 ,\n",
       "         1.2785032 ,  1.2187283 ,  1.1583496 ,  1.0973611 ,  1.0357565 ,\n",
       "         0.97352946,  0.910674  ,  0.84718364,  0.78305197,  0.71827245,\n",
       "         0.6528385 ,  0.58674365,  0.51998127,  0.4525445 ,  0.38442665,\n",
       "         0.3156207 ,  0.24611972,  0.17591675,  0.1050045 ,  0.03337599,\n",
       "        -0.03897607, -0.1120588 , -0.18587987, -0.26044658, -0.33576638,\n",
       "        -0.4118471 , -0.48869628, -0.5663217 , -0.6447312 , -0.72393274,\n",
       "        -0.80393434, -0.884744  , -0.96636987, -1.0488203 , -1.1321034 ,\n",
       "        -1.2162279 , -1.3012022 , -1.3870347 , -1.4737343 , -1.5613096 ,\n",
       "        -1.6497694 , -1.7391229 , -1.8293788 ], dtype=float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = result[2]\n",
    "get_returns(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a873d05",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88c5219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.635096Z",
     "start_time": "2023-03-23T16:03:05.632465Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "def calculate_loss(returns, values, value_next, discount_array):\n",
    "    \"\"\"\n",
    "    Policy part ---------\n",
    "    V(St) = E(Q) = Pi(St, At1; theta) * Q(St, At1) + ...\n",
    "    G denote gradient W.R.T theta\n",
    "    \n",
    "    \n",
    "    G(V(St)) = G[Pi(St, At1; theta) * Q(St, At1) + ...]\n",
    "                approximate= Pi(St, A1)* G(logpi(St, At1; theta) * Q) + ...  # chain rule G(logpi) = 1/pi * G(pi)\n",
    "                = E[ G(logpi * Q) ] # Pi(St, A) is the PDF, so this is the expectation\n",
    "              [1]  approximate= G(logpi * Q)  # monte carlo approximation\n",
    "              [2]  = G(logpi * (Q - baseline))  where baseline can be V. This is the A2C\n",
    "                  Qt can be approximate by Yt\n",
    "              Yt = gamma^T * Q(T) + r + gamma*r + gamma^2*r + ...\n",
    "              \n",
    "    Critic Part TD learning -----------\n",
    "    Qt = discounted_ovserved + QT\n",
    "            \n",
    "    \"\"\"\n",
    "    values = tf.squeeze(values)\n",
    "    Yt = tf.squeeze(returns) + discount_array * value_next\n",
    "    \n",
    "    loss = loss_func(tf.expand_dims(values, 1), tf.expand_dims(Yt,1))\n",
    "    \n",
    "    return loss#, abs(Yt - values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b72202c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.660688Z",
     "start_time": "2023-03-23T16:03:05.636017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.99447125>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, actions, rewards, next_value, state, done = result\n",
    "discount_array, returns_array = get_returns(rewards, gamma)\n",
    "calculate_loss(returns_array, values, next_value, discount_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a2689",
   "metadata": {},
   "source": [
    "# train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea4f1820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:05.665731Z",
     "start_time": "2023-03-23T16:03:05.661601Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "@tf.function\n",
    "def run_train_step(model, optimizer, start_state, step_length):\n",
    "    with tf.GradientTape() as tape:\n",
    "        STEP_RES = run_step(start_state, model, step_length)\n",
    "        values, actions, rewards, next_value, state, done = STEP_RES\n",
    "        discount_array, returns_array = get_returns(rewards, gamma)\n",
    "        loss = calculate_loss(returns_array, values, next_value, discount_array)\n",
    "    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "    return STEP_RES, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecf721e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:03:07.287014Z",
     "start_time": "2023-03-23T16:03:05.666650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(28,), dtype=float32, numpy=\n",
       "  array([0.00719846, 0.00546249, 0.00754609, 0.00549177, 0.00784724,\n",
       "         0.00557674, 0.0085881 , 0.00587735, 0.00985033, 0.00628006,\n",
       "         0.01065368, 0.00670577, 0.01065022, 0.00744282, 0.0106605 ,\n",
       "         0.00854854, 0.01098789, 0.00979421, 0.01157524, 0.01098666,\n",
       "         0.01211687, 0.01234939, 0.01250588, 0.01354526, 0.01299097,\n",
       "         0.01507375, 0.01358637, 0.01682195], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(28,), dtype=int32, numpy=\n",
       "  array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(28,), dtype=float32, numpy=\n",
       "  array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.014323888>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.02062696,  0.01342651,  0.21177892,  0.70574605], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(), dtype=bool, numpy=True>),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0034688>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state, _ = env.reset()\n",
    "run_train_step(model, optimizer, _state, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4a1c9",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3289955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.441058Z",
     "start_time": "2023-03-23T16:03:07.288175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 182/10000 [00:08<09:46, 16.73it/s, current_reward=92, loss=1, running_rewards=90.7]     2023-03-23 16:03:15.622242: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  3%|▎         | 255/10000 [00:13<15:18, 10.61it/s, current_reward=158, loss=0.999, running_rewards=122] 2023-03-23 16:03:20.589857: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  3%|▎         | 270/10000 [00:14<11:02, 14.68it/s, current_reward=61, loss=0.977, running_rewards=124] 2023-03-23 16:03:21.542435: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  3%|▎         | 288/10000 [00:15<08:38, 18.72it/s, current_reward=47, loss=0.954, running_rewards=120] 2023-03-23 16:03:22.703760: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  4%|▎         | 367/10000 [00:20<08:03, 19.92it/s, current_reward=360, loss=1, running_rewards=125]    2023-03-23 16:03:27.953732: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  4%|▍         | 431/10000 [00:24<09:08, 17.45it/s, current_reward=94, loss=0.939, running_rewards=129] 2023-03-23 16:03:32.148088: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  6%|▌         | 573/10000 [00:33<16:44,  9.38it/s, current_reward=288, loss=0.911, running_rewards=121]2023-03-23 16:03:41.307452: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  6%|▋         | 641/10000 [00:38<09:12, 16.94it/s, current_reward=246, loss=0.97, running_rewards=117] 2023-03-23 16:03:45.719554: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  7%|▋         | 665/10000 [00:39<06:17, 24.72it/s, current_reward=37, loss=0.965, running_rewards=97.4]2023-03-23 16:03:46.612651: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  7%|▋         | 746/10000 [00:42<05:18, 29.08it/s, current_reward=55, loss=0.894, running_rewards=57.4] 2023-03-23 16:03:49.475044: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  8%|▊         | 766/10000 [00:42<03:55, 39.22it/s, current_reward=39, loss=0.961, running_rewards=54.5]2023-03-23 16:03:49.923968: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  8%|▊         | 797/10000 [00:43<04:22, 34.99it/s, current_reward=49, loss=0.959, running_rewards=49.2]2023-03-23 16:03:50.854812: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "  9%|▉         | 934/10000 [00:47<04:05, 36.88it/s, current_reward=34, loss=0.986, running_rewards=41]   2023-03-23 16:03:54.796499: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 11%|█         | 1096/10000 [00:52<04:32, 32.66it/s, current_reward=34, loss=0.936, running_rewards=46.4] 2023-03-23 16:03:59.823636: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 13%|█▎        | 1284/10000 [00:57<03:41, 39.26it/s, current_reward=32, loss=0.929, running_rewards=39.2] 2023-03-23 16:04:04.977243: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 14%|█▍        | 1447/10000 [01:02<03:58, 35.90it/s, current_reward=26, loss=0.912, running_rewards=41.4] 2023-03-23 16:04:09.405845: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 16%|█▋        | 1633/10000 [01:06<03:20, 41.80it/s, current_reward=23, loss=0.893, running_rewards=37.3] 2023-03-23 16:04:14.322085: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 17%|█▋        | 1706/10000 [01:08<03:41, 37.44it/s, current_reward=27, loss=0.891, running_rewards=37.1] 2023-03-23 16:04:16.279530: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 18%|█▊        | 1754/10000 [01:10<03:44, 36.74it/s, current_reward=24, loss=0.885, running_rewards=38.5]2023-03-23 16:04:17.532002: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 18%|█▊        | 1803/10000 [01:11<03:29, 39.10it/s, current_reward=52, loss=0.754, running_rewards=38.2]2023-03-23 16:04:18.998228: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 18%|█▊        | 1822/10000 [01:12<03:53, 35.05it/s, current_reward=42, loss=0.89, running_rewards=39.6] 2023-03-23 16:04:19.557775: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      " 19%|█▊        | 1853/10000 [01:12<05:20, 25.41it/s, current_reward=69, loss=0.891, running_rewards=38.8]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_epoch = 100\n",
    "max_epoch = 10000\n",
    "step_length = 50\n",
    "thred = 475\n",
    "running_rewards = collections.deque(maxlen=100)\n",
    "max_steps_per_epoch = 500\n",
    "all_rewards = []\n",
    "all_running_rewards = []\n",
    "t = tqdm.trange(max_epoch)\n",
    "for i in t:\n",
    "    restart = False\n",
    "    start_state, _ = env.reset()\n",
    "    cur_step = 0\n",
    "    epoch_reward = 0\n",
    "    while cur_step < max_steps_per_epoch:\n",
    "        try:\n",
    "            STEP_RES, loss= run_train_step(model, optimizer, start_state, step_length)\n",
    "        except Exception:\n",
    "            restart = True\n",
    "            break\n",
    "        cur_step += step_length\n",
    "        values, actions, rewards, next_value, state, done = STEP_RES\n",
    "        epoch_reward += int(sum(rewards))\n",
    "        if done:\n",
    "            break\n",
    "    if restart:\n",
    "        continue\n",
    "    running_rewards.append(epoch_reward)\n",
    "    avg_reward = statistics.mean(running_rewards)\n",
    "    all_rewards.append(epoch_reward)\n",
    "    all_running_rewards.append(avg_reward)\n",
    "    t.set_postfix(running_rewards=avg_reward, current_reward=epoch_reward, loss=float(loss))\n",
    "    if avg_reward > thred and i > min_epoch:\n",
    "        break\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a06a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.442318Z",
     "start_time": "2023-03-23T16:04:20.442310Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d141f82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.442867Z",
     "start_time": "2023-03-23T16:04:20.442860Z"
    }
   },
   "outputs": [],
   "source": [
    "_state, _ = env.reset()\n",
    "result = run_step(_state, model, 1000)\n",
    "values, actions, rewards, next_value, state, done = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b502d8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.443652Z",
     "start_time": "2023-03-23T16:04:20.443644Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(values)), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2eaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.444258Z",
     "start_time": "2023-03-23T16:04:20.444250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "\n",
    "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "  state, info = env.reset()\n",
    "  state = tf.constant(state, dtype=tf.float32)\n",
    "  screen = env.render()\n",
    "  images = [Image.fromarray(screen)]\n",
    "  values = []\n",
    "\n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    value_output = model(state)\n",
    "    value_output = np.squeeze(value_output)\n",
    "    action = np.argmax(np.squeeze(value_output))\n",
    "    values.append(value_output[action])\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 10 steps\n",
    "    if i % 10 == 0:\n",
    "      screen = env.render()\n",
    "      images.append(Image.fromarray(screen))\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  return images, values\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images,values = render_episode(render_env, model, max_steps_per_epoch)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bf217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.445155Z",
     "start_time": "2023-03-23T16:04:20.445147Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2972fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:04:20.445624Z",
     "start_time": "2023-03-23T16:04:20.445616Z"
    }
   },
   "outputs": [],
   "source": [
    "values = tf.stack(values)\n",
    "values = values.numpy()\n",
    "plt.plot(range(len(values)), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc62dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf211",
   "language": "python",
   "name": "tf211"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
